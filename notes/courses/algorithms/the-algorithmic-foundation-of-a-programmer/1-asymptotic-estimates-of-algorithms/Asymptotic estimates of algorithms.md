
---
## Асимптотические оценки
_Определение_:
- $f(n)\ =\ O(g(n))$, если существует такая константа $c\ >\ 0$ и такое чисто $n_{0}$, что $0 \leq f(n) \leq cg(n)$ для всех $n \geq n_{0}$.
- "Эф от эн есть о большое от же от эн".

$O(n)$ - верхняя оценка алгоритма

---
## Стандартные асимптотики
### $O(1)$:
$O(1)$ - время работы не зависит от размера входа.
_Пример_:
- Арифметические операции.
- Индексация массива.
- Обращение к вершине кучу.
- Вставка в двусвязный список.

### $O(log N)$:
$O(log N)$ - время работы зависит от размера входа и действий в алгоритме будет **меньше**, чем количество входных данных. Основанием логарифма мы пренебрегаем, так как логарифмы по различным основаниям различаются лишь константой перед самим логарифмом. А при оценке асимптотики, мы константами принебрегаем.
_Пример_:
- Двоичный поиск в отсортированном массиве.
- Поиск/вставка/удаление в `std::set`/`TreeSet`.
- Извлечение элемента их вершины кучи.

### $O(N)$:
$O(N)$ - время работы зависит от размера входа линейно и действий в алгоритме будет **столько же**, сколько входных данных.
_Пример_:
- Итерация по всем элементам массива.
- Поиск минимума/максимума в неотсортированном массиве.
- Обход дерева из $N$ вершин.
- Построение кучи.

### $O(N\ logN)$:
$O(N\ logN)$ - время работы зависит от размера входа и действий в алгоритме будет **больше в** $logN$ раз, чем количество входных данных.
_Пример_:
- Сортировка слиянием (merge sort).
- Сортировка кучей массива из $N$ элементов.

### $O(N^{2})$:
$O(N^{2})$  - время работы зависит от размера входа и действий в алгоритме будет **больше в** $N$ раз, чем количество входных данных. 
$N^2$ так же можно разложить как $1 + 2 + 3 + \cdots + (N - 1)$ .
_Пример_:
- Сортировка вставками

### $O(2^{N})$:
$O(2^{N})$  - время работы зависит от размера входа и действий в алгоритме будет **больше**, чем количество входных данных.
_Пример_:
- Перебор всех подмножеств набора из $N$ элементов. 

### $O(N!)$:
$O(N!)$  - время работы зависит от размера входа и действий в алгоритме будет **больше**, чем количество входных данных.
_Пример_:
- Перебор всех подмножеств набора из $N$ элементов.

---

![[general-asymptotics.png]]
$O(1)\ <\  O(logN)\ <\  O(\sqrt{N})\ <\  O(N)\ <\  O(N\ logN)\ <\  O(N^{2})\ <\  O(N^{3})\ <\  O(2^{N})\ <\  O(N!)\ <\  O(N^N)$
---
## Сложение асимптотик
Главное правило тут очень простое - если алгоритм имеет несколько кусков с разными асимптотиками, то они складываются по правилу: "Побеждает большее", или другими словами - большее слогаемое всегда поглащает меньшее.
То есть:
- $O(N) + O(log N)\ =\ O(N)$
-  $O(1) + O(N\ log N)\ =\ O(N\ log N)$
- $O(N^{3}) + O(N) + O(N\ logN)\ =\ O(N^{3})$
- $O(N^{30}) + O(2^{N})\ =\ O(2^{N})$
### Всегда ли одно слагаемое остается в итоге при сложении?
- Короткий ответ - нет.
- _Пример:_ Поиск в глубину в графе и $V$ вершин и $E$ ребер имеет асимптотику $O(V + E)$. Это получается из-за того, что мы явно не может определеить кто больше $V < E$ или же $V > E$. То есть, если мы не знаем какое из слагаемых больше - мы оставляем оба со знаком плюса.

---
## Умножение асимптотик
Если асимптотика умножается на константное число, то просто происходит поглащение константы по правилу:
- $3 * O(N) = O(N)$
- $\forall C\ C * O(f(n))\ =\ O(f(n))$

Но при этом, часть алгоритма имеет сложность $O(M)$, и на каждую из итераций происходит действие, которое имеет сложность $O(logN)$, то данные асимптотики перемножатся и мы получим в итоге сложность - $O(M\ logN)$, но так будет только при условии, что мы знаем, что $M\ >\ N$.
Формально это выглядит так:
- $M * O(f(n))\ =\ O(M * f(n))$
- $O(N) * O(logK)\ =\ O(N\ logK)$
---
## Оценка времени работы алгоритма (в реальных единицах)
1. Рассматриваем худший случай.
2. Вычисляем количество операций при максимальном $N$.
3. Умножаем на магическую константу ~10-50.
4. Считаем что за секунду можно сделать $10^{9}$ операций.
---
Так же существуют и другие оценки сложности работы алгоритмов (но обычно они не пригаждаются на практике):
- Нотация большого $O$:
	$0 \leq f(n) \leq cg(n)\ \forall\ n \geq n_{0}$, то есть $O$ - это верхняя граница оценки (можно представить как $\leq$).
	
	![[big-o-notation.png]]
- Нотация большой $\Omega$:
	$0 \leq cg(n) \leq f(n)\ \forall \ n \geq n_{0}$, то есть $\Omega$ - это нижняя граница оценки (можно представить как $\geq$).
	
	![[big-omega-notation.png]]
- Нотация большой $\Theta$:
	$0 \leq f(n) \leq c_{1}g(n)\ \forall \ n \geq n_{0},\ и\ 0 \leq c_{2}g(n) \leq f(n) \ \forall n \geq n_{0}$, и записав это как единое выражение, получим: $0 \leq c_{2}g(n) \leq f(n) \leq c_{1}g(n)\ \forall \ n \geq n0$, то есть $\Theta$ - это малый диапазон границ оценки (можно представить как $=$).
	![[big-theta-notation.png]]
	