
---
## Асимптотические оценки
_Определение_:
- $f(n)\ =\ O(g(n))$, если существует такая константа $c\ >\ 0$ и такое чисто $n_{0}$, что $0 \leq f(n) \leq cg(n)$ для всех $n \geq n_{0}$.
- "Эф от  эн есть о большое от же от эн".

$O(n)$ - верхняя оценка алгоритма
$o(n)$ - нижняя оценка алгоритма

---
## Стандартные асимптотики
### $O(1)$:
$O(1)$ - время работы не зависит от размера входа.
_Пример_:
- Арифметические операции.
- Индексация массива.
- Обращение к вершине кучу.
- Вставка в двусвязный список.

### $O(log N)$:
$O(log N)$ - время работы зависит от размера входа и действий в алгоритме будет **меньше**, чем количество входных данных. Основанием логарифма мы пренебрегаем, так как логарифмы по различным основаниям различаются лишь константой перед самим логарифмом. А при оценке асимптотики, мы константами принебрегаем.
_Пример_:
- Двоичный поиск в отсортированном массиве.
- Поиск/вставка/удаление в `std::set`/`TreeSet`.
- Извлечение элемента их вершины кучи.

### $O(N)$:
$O(N)$ - время работы зависит от размера входа линейно и действий в алгоритме будет **столько же**, сколько входных данных.
_Пример_:
- Итерация по всем элементам массива.
- Поиск минимума/максимума в неотсортированном массиве.
- Обход дерева из $N$ вершин.
- Построение кучи.

### $O(N\ logN)$:
$O(N\ logN)$ - время работы зависит от размера входа и действий в алгоритме будет **больше в** $logN$ раз, чем количество входных данных.
_Пример_:
- Сортировка слиянием (merge sort).
- Сортировка кучей массива из $N$ элементов.

### $O(N^{2})$:
$O(N^{2})$  - время работы зависит от размера входа и действий в алгоритме будет **больше в** $N$ раз, чем количество входных данных. 
$N^2$ так же можно разложить как $1 + 2 + 3 + \cdots + (N - 1)$ .
_Пример_:
- Сортировка вставками

### $O(2^{N})$:
$O(2^{N})$  - время работы зависит от размера входа и действий в алгоритме будет **больше**, чем количество входных данных.
_Пример_:
- Перебор всех подмножеств набора из $N$ элементов. 

### $O(N!)$:
$O(N!)$  - время работы зависит от размера входа и действий в алгоритме будет **больше**, чем количество входных данных.
_Пример_:
- Перебор всех подмножеств набора из $N$ элементов.

---

![[general-asymptotics.png]]
$O(1)\ <\  O(logN)\ <\  O(\sqrt{N})\ <\  O(N)\ <\  O(N\ logN)\ <\  O(N^{2})\ <\  O(N^{3})\ <\  O(2^{N})\ <\  O(N!)\ <\  O(N^N)$
---
## Сложение асимптотик
Главное правило тут очень простое - если алгоритм имеет несколько кусков с разными асимптотиками, то они складываются по правилу: "Побеждает большее", или другими словами - большее слогаемое всегда поглащает меньшее.
То есть:
- $O(N) + O(log N)\ =\ O(N)$
-  $O(1) + O(N\ log N)\ =\ O(N\ log N)$
- $O(N^{3}) + O(N) + O(N\ logN)\ =\ O(N^{3})$
- $O(N^{30}) + O(2^{N})\ =\ O(2^{N})$
### Всегда ли одно слагаемое остается в итоге при сложении?
- Короткий ответ - нет.
- _Пример:_ Поиск в глубину в графе и $V$ вершин и $E$ ребер имеет асимптотику $O(V + E)$. Это получается из-за того, что мы явно не может определеить кто больше $V < E$ или же $V > E$. То есть, если мы не знаем какое из слагаемых больше - мы оставляем оба со знаком плюса.

---
## Умножение асимптотик
Если асимптотика умножается на константное число, то просто происходит поглащение константы по правилу:
- $3 * O(N) = O(N)$
- $\forall C\ C * O(f(n))\ =\ O(f(n))$

Но при этом, часть алгоритма имеет сложность $O(M)$, и на каждую из итераций происходит действие, которое имеет сложность $O(logN)$, то данные асимптотики перемножатся и мы получим в итоге сложность - $O(M\ logN)$, но так будет только при условии, что мы знаем, что $M\ >\ N$.
Формально это выглядит так:
- $M * O(f(n))\ =\ O(M * f(n))$
- $O(N) * O(logK)\ =\ O(N\ logK)$
---
## Оценка времени работы алгоритма (в реальных единицах)
1. Рассматриваем худший случай.
2. Вычисляем количество операций при максимальном $N$.
3. Умножаем на магическую константу ~10-50.
4. Считаем что за секунду можно сделать $10^{9}$ операций.